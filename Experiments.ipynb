{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcabc754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "base_dir = \"./privilage_analysis_data/data_cve/\"\n",
    "\n",
    "trace_parser = [\"sub_sys\", \"sub_ip\", \"obj_ip\", \"cs\"]\n",
    "\n",
    "all_traces_cve_df = pd.DataFrame()\n",
    "\n",
    "for idx, fileLocation in enumerate(os.listdir(base_dir)):\n",
    "\n",
    "    filePath = os.path.join(base_dir, fileLocation, \"trace.txt\")\n",
    "    if not os.path.isfile(filePath):\n",
    "        continue\n",
    "    print(filePath)\n",
    "    \n",
    "    df = pd.read_csv(filePath, names=trace_parser, skiprows=[0])\n",
    "    df['trace_name'] = fileLocation\n",
    "    df = df[(df['sub_sys']!=-1)]\n",
    "    df = df[(df['obj_ip']!='eeeeffff')]\n",
    "    df = df[(df['obj_ip']!='feedbeef')]\n",
    "    df['tuple'] = 'ss' + df['sub_sys'].astype(str) + 'os' + df['obj_ip'].astype(str)\n",
    "\n",
    "    #print(df.info())\n",
    "    df['index'] = df.reset_index().index.to_list()\n",
    "        \n",
    "    df.dropna(inplace=True)\n",
    "        \n",
    "    df['float_obj_ip'] = df['obj_ip'].astype(str).apply(int, base=16)\n",
    "\n",
    "    df['deltas'] = abs(df['float_obj_ip'] - df['float_obj_ip'].shift(1))\n",
    "    \n",
    "    df['deltas'] = df['deltas'].astype(str)\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    all_traces_cve_df = pd.concat([all_traces_cve_df, df])\n",
    "\n",
    "#all_traces_cve_df.reset_index(inplace=True)\n",
    "print(all_traces_cve_df.shape)\n",
    "print(all_traces_cve_df.info())\n",
    "all_traces_cve_df = all_traces_cve_df[~all_traces_cve_df.obj_ip.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d0c7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "base_dir = \"./privilage_analysis_data/data_all/\"\n",
    "\n",
    "trace_parser = [\"sub_sys\", \"sub_ip\", \"obj_ip\", \"cs\"]\n",
    "\n",
    "all_traces_df = pd.DataFrame()\n",
    "\n",
    "for idx, fileLocation in enumerate(os.listdir(base_dir)):\n",
    "\n",
    "    filePath = os.path.join(base_dir, fileLocation, \"trace.txt\")\n",
    "    if not os.path.isfile(filePath):\n",
    "        continue\n",
    "    print(filePath)\n",
    "    \n",
    "    df = pd.read_csv(filePath, names=trace_parser, skiprows=[0])\n",
    "    df['trace_name'] = fileLocation\n",
    "    #print(df.info())\n",
    "    df = df[(df['sub_sys']!=-1)]\n",
    "    df = df[(df['obj_ip']!='eeeeffff')]\n",
    "    df = df[(df['obj_ip']!='feedbeef')]\n",
    "    df['tuple'] = 'ss' + df['sub_sys'].astype(str) + 'os' + df['obj_ip'].astype(str)\n",
    "    \n",
    "    df['index'] = df.reset_index().index.to_list()\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "        \n",
    "    df['float_obj_ip'] = df['obj_ip'].astype(str).apply(int, base=16)\n",
    "\n",
    "    df['deltas'] = abs(df['float_obj_ip'] - df['float_obj_ip'].shift(1))\n",
    "    \n",
    "    df['deltas'] = df['deltas'].astype(str)\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    all_traces_df = pd.concat([all_traces_df, df])\n",
    "\n",
    "#all_traces_df.reset_index(inplace=True)\n",
    "print(all_traces_df.shape)\n",
    "print(all_traces_df.info())\n",
    "all_traces_df = all_traces_df[~all_traces_df.obj_ip.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a44257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000,2000, 6000, 8000, 10000\n",
    "sequence_length = 8000\n",
    "test_size=0.20 \n",
    "random_state=42\n",
    "# 1, 20, 50, 80, 100, 200\n",
    "sentence_length = 100\n",
    "epochs=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712a3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint  # pretty-printer\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "## vectorize\n",
    "all_df = pd.concat([all_traces_df,  all_traces_cve_df])\n",
    "all_df.shape\n",
    "\n",
    "vectorized_traces = pd.DataFrame()\n",
    "vectorized_traces = all_df.groupby(['trace_name'], as_index=False).count()[['trace_name', 'index']]\n",
    "sequenced_traces = all_df.groupby(['trace_name'], as_index=False)['obj_ip'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "vectorized_traces = pd.merge(\n",
    "    sequenced_traces,\n",
    "    vectorized_traces,\n",
    "    how=\"inner\",\n",
    "    on='trace_name',\n",
    "    sort=True,\n",
    ")\n",
    "vectorized_traces\n",
    "\n",
    "vectorized_traces['trace_label'] = vectorized_traces['trace_name'].str.split('-').str[0]\n",
    "vectorized_traces['label'] = vectorized_traces[\"trace_label\"].astype('category').cat.codes\n",
    "vectorized_traces['binary_label'] = np.where(vectorized_traces['trace_label'].str.contains('CVE'), 0, 1)\n",
    "\n",
    "a, b = train_test_split(vectorized_traces[vectorized_traces['binary_label'] ==1], test_size=0.50, random_state=2, stratify=vectorized_traces[vectorized_traces['binary_label'] ==1]['trace_label']) \n",
    "at = vectorized_traces[vectorized_traces['binary_label'] ==0].copy()\n",
    "vectorized_traces = pd.concat([at, a])\n",
    "training_traces = vectorized_traces.iloc[51:]\n",
    "\n",
    "testing_traces = vectorized_traces.iloc[0:50]\n",
    "testing_traces\n",
    "training_traces.groupby(['trace_label'], as_index=False).count()\n",
    "\n",
    "#-----------\n",
    "\n",
    "binary_labels = vectorized_traces['binary_label']\n",
    "X_train, X_test, y_train, y_test, train_index, test_index = train_test_split(vectorized_traces, binary_labels, vectorized_traces.index.to_list(), test_size=0.20, random_state=42, stratify=binary_labels) \n",
    "\n",
    "df1, df2 = X_train[(mask:=(X_train['trace_name'].str.contains('CVE_2022_31440')) | (X_train['trace_name'].str.contains('hdparm')))].copy(), X_train[~mask].copy()\n",
    "\n",
    "vectorized_traces[vectorized_traces['binary_label'] == 1]\n",
    "\n",
    "tokenized_all_traces = [\n",
    "    [word for word in trace.split()]\n",
    "    for trace in vectorized_traces['obj_ip'].to_numpy()\n",
    "]\n",
    "\n",
    "tokenized_traces = [\n",
    "    [word for word in trace.split()]\n",
    "    for trace in training_traces['obj_ip'].to_numpy()\n",
    "]\n",
    "\n",
    "tokenized_train_traces = [\n",
    "    [word for word in trace.split()]\n",
    "    for trace in X_train['obj_ip'].to_numpy()\n",
    "]\n",
    "\n",
    "tokenized_test_traces = [\n",
    "    [word for word in trace.split()]\n",
    "    for trace in testing_traces['obj_ip'].to_numpy()\n",
    "]\n",
    "\n",
    "tokenized_test_model_traces = [\n",
    "    [word for word in trace.split()]\n",
    "    for trace in X_test['obj_ip'].to_numpy()\n",
    "]\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "## tokenize text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_traces + tokenized_test_traces)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)\n",
    "vocab_size\n",
    "\n",
    "## padding data\n",
    "#sequence_length = 8000\n",
    "\n",
    "all_sequences = tokenizer.texts_to_sequences(tokenized_all_traces)\n",
    "padded_all_seq = pad_sequences(all_sequences, maxlen=sequence_length, padding='pre', truncating='post')\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(tokenized_traces)\n",
    "padded_train_seq = pad_sequences(train_sequences, maxlen=sequence_length, padding='pre', truncating='post')\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(tokenized_test_traces)\n",
    "padded_test_seq = pad_sequences(test_sequences, maxlen=sequence_length, padding='pre', truncating='post')\n",
    "\n",
    "train_model_sequences = tokenizer.texts_to_sequences(tokenized_train_traces)\n",
    "padded_train_model_seq = pad_sequences(train_model_sequences, maxlen=sequence_length, padding='pre', truncating='post')\n",
    "\n",
    "test_model_sequences = tokenizer.texts_to_sequences(tokenized_test_model_traces)\n",
    "padded_test_model_seq = pad_sequences(test_model_sequences, maxlen=sequence_length, padding='pre', truncating='post')\n",
    "\n",
    "print(padded_train_model_seq.shape, padded_test_model_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e8a23c",
   "metadata": {},
   "source": [
    "## Whole Seq w=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438cb3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from numpy import array_equal\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "#sentence_length = 1\n",
    "sentence_num = padded_train_model_seq.shape[1]//sentence_length\n",
    "batch_size = padded_train_model_seq.shape[0]\n",
    "whole_seq_length = padded_train_model_seq.shape[1]\n",
    "\n",
    "padded_train_model_seq_reshaped = np.reshape(padded_train_model_seq, (batch_size, sentence_num, sentence_length, 1))\n",
    "\n",
    "#inputs = Input(shape=(100, 50, 1))\n",
    "#lstm_sentence_layer = LSTM(128)\n",
    "#sentence_inputs = TimeDistributed(lstm_sentence_layer)(inputs)\n",
    "#sentence_inputs.shape\n",
    "\n",
    "ms = Sequential()\n",
    "ms.add(Input(shape=(whole_seq_length, 1)))\n",
    "#ms.add(TimeDistributed(Bidirectional(LSTM(128))))\n",
    "#ms.add(Dropout(0.2))\n",
    "ms.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "ms.add(Dropout(0.2))\n",
    "ms.add(LSTM(128))\n",
    "ms.add(Dropout(0.2))\n",
    "ms.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "#m = Sequential([\n",
    "#    #Input(tensor=(100, 50, 1)),\n",
    "#    TimeDistributed(Bidirectional(LSTM(128))),\n",
    "#    Dropout(0.2),\n",
    "#    Bidirectional(LSTM(128, return_sequences=True, stateful=True)),\n",
    "#    Dropout(0.2),\n",
    "#    LSTM(128, stateful=True),\n",
    "#    Dropout(0.2),\n",
    "#    Dense(28, activation='softmax')\n",
    "#    #,\n",
    "#    #Dropout(0.2),\n",
    "#    #Dense(256),\n",
    "#    #Dense(1, activation='sigmoid')\n",
    "#])\n",
    "\n",
    "\n",
    "mod = Model()\n",
    "\n",
    "ms.compile(loss='binary_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "ms.summary()\n",
    "\n",
    "history = ms.fit(padded_train_model_seq_reshaped, y_train, epochs=epochs)\n",
    "\n",
    "ms.save('w'+str(sentence_length)+'_l'+str(sequence_length)+'_no_max_pooling_no_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf225b4",
   "metadata": {},
   "source": [
    "## Whole Seq w=1 with Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85000f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from numpy import array_equal\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Dropout, Embedding\n",
    "\n",
    "#sentence_length = 1\n",
    "sentence_num = padded_train_model_seq.shape[1]//sentence_length\n",
    "batch_size = padded_train_model_seq.shape[0]\n",
    "whole_seq_length = padded_train_model_seq.shape[1]\n",
    "\n",
    "padded_train_model_seq_reshaped = np.reshape(padded_train_model_seq, (batch_size, sentence_num, sentence_length, 1))\n",
    "\n",
    "#inputs = Input(shape=(100, 50, 1))\n",
    "#lstm_sentence_layer = LSTM(128)\n",
    "#sentence_inputs = TimeDistributed(lstm_sentence_layer)(inputs)\n",
    "#sentence_inputs.shape\n",
    "\n",
    "ms = Sequential()\n",
    "ms.add(Input(shape=(whole_seq_length, 1)))\n",
    "ms.add(Embedding(vocab_size, 64))\n",
    "#ms.add(TimeDistributed(Bidirectional(LSTM(128))))\n",
    "#ms.add(Dropout(0.2))\n",
    "ms.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "ms.add(Dropout(0.2))\n",
    "ms.add(LSTM(128))\n",
    "ms.add(Dropout(0.2))\n",
    "ms.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "#m = Sequential([\n",
    "#    #Input(tensor=(100, 50, 1)),\n",
    "#    TimeDistributed(Bidirectional(LSTM(128))),\n",
    "#    Dropout(0.2),\n",
    "#    Bidirectional(LSTM(128, return_sequences=True, stateful=True)),\n",
    "#    Dropout(0.2),\n",
    "#    LSTM(128, stateful=True),\n",
    "#    Dropout(0.2),\n",
    "#    Dense(28, activation='softmax')\n",
    "#    #,\n",
    "#    #Dropout(0.2),\n",
    "#    #Dense(256),\n",
    "#    #Dense(1, activation='sigmoid')\n",
    "#])\n",
    "\n",
    "\n",
    "mod = Model()\n",
    "\n",
    "ms.compile(loss='binary_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "ms.summary()\n",
    "\n",
    "history = ms.fit(padded_train_model_seq_reshaped, y_train, epochs=epochs)\n",
    "\n",
    "ms.save('w'+str(sentence_length)+'_l'+str(sequence_length)+'_no_max_pooling_no_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8805fbf",
   "metadata": {},
   "source": [
    "## Sliced Seq w != 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe71e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from numpy import array_equal\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "#sentence_length = 100\n",
    "sentence_num = padded_train_model_seq.shape[1]//sentence_length\n",
    "batch_size = padded_train_model_seq.shape[0]\n",
    "whole_seq_length = padded_train_model_seq.shape[1]\n",
    "\n",
    "padded_train_model_seq_reshaped = np.reshape(padded_train_model_seq, (batch_size, sentence_num, sentence_length, 1))\n",
    "\n",
    "#inputs = Input(shape=(100, 50, 1))\n",
    "#lstm_sentence_layer = LSTM(128)\n",
    "#sentence_inputs = TimeDistributed(lstm_sentence_layer)(inputs)\n",
    "#sentence_inputs.shape\n",
    "ms.reset_states()\n",
    "\n",
    "ms = Sequential()\n",
    "ms.add(Input(shape=(sentence_num, sentence_length, 1)))\n",
    "ms.add(TimeDistributed(Bidirectional(LSTM(128))))\n",
    "ms.add(Dropout(0.2))\n",
    "ms.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "ms.add(Dropout(0.2))\n",
    "ms.add(LSTM(128))\n",
    "ms.add(Dropout(0.2))\n",
    "ms.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "#m = Sequential([\n",
    "#    #Input(tensor=(100, 50, 1)),\n",
    "#    TimeDistributed(Bidirectional(LSTM(128))),\n",
    "#    Dropout(0.2),\n",
    "#    Bidirectional(LSTM(128, return_sequences=True, stateful=True)),\n",
    "#    Dropout(0.2),\n",
    "#    LSTM(128, stateful=True),\n",
    "#    Dropout(0.2),\n",
    "#    Dense(28, activation='softmax')\n",
    "#    #,\n",
    "#    #Dropout(0.2),\n",
    "#    #Dense(256),\n",
    "#    #Dense(1, activation='sigmoid')\n",
    "#])\n",
    "\n",
    "\n",
    "mod = Model()\n",
    "\n",
    "ms.compile(loss='binary_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "ms.summary()\n",
    "\n",
    "\n",
    "history = ms.fit(padded_train_model_seq_reshaped, y_train, epochs=40)\n",
    "\n",
    "#ms.save('w'+str(sentence_length)+'_l'+str(sequence_length)+'_no_max_pooling_no_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6985b3",
   "metadata": {},
   "source": [
    "## Sliced Seq with Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823acc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.evaluate(padded_test_model_seq_reshaped, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac04574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from numpy import array_equal\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Dropout, Embedding\n",
    "\n",
    "#sentence_length = 100\n",
    "sentence_num = padded_train_model_seq.shape[1]//80\n",
    "batch_size = padded_train_model_seq.shape[0]\n",
    "whole_seq_length = padded_train_model_seq.shape[1]\n",
    "\n",
    "padded_train_model_seq_reshaped = np.reshape(padded_train_model_seq, (batch_size, sentence_num, 80, 1))\n",
    "\n",
    "#inputs = Input(shape=(100, 50, 1))\n",
    "#lstm_sentence_layer = LSTM(128)\n",
    "#sentence_inputs = TimeDistributed(lstm_sentence_layer)(inputs)\n",
    "#sentence_inputs.shape\n",
    "embed_ms.reset_states()\n",
    "\n",
    "\n",
    "embed_ms = Sequential()\n",
    "embed_ms.add(Input(shape=(sentence_num, 80)))\n",
    "embed_ms.add(Embedding(vocab_size, 64))\n",
    "embed_ms.add(TimeDistributed(Bidirectional(LSTM(128))))\n",
    "embed_ms.add(Dropout(0.2))\n",
    "embed_ms.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "embed_ms.add(Dropout(0.2))\n",
    "embed_ms.add(LSTM(128))\n",
    "embed_ms.add(Dropout(0.2))\n",
    "embed_ms.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "#m = Sequential([\n",
    "#    #Input(tensor=(100, 50, 1)),\n",
    "#    TimeDistributed(Bidirectional(LSTM(128))),\n",
    "#    Dropout(0.2),\n",
    "#    Bidirectional(LSTM(128, return_sequences=True, stateful=True)),\n",
    "#    Dropout(0.2),\n",
    "#    LSTM(128, stateful=True),\n",
    "#    Dropout(0.2),\n",
    "#    Dense(28, activation='softmax')\n",
    "#    #,\n",
    "#    #Dropout(0.2),\n",
    "#    #Dense(256),\n",
    "#    #Dense(1, activation='sigmoid')\n",
    "#])\n",
    "\n",
    "\n",
    "mod = Model()\n",
    "\n",
    "embed_ms.compile(loss='binary_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "embed_ms.summary()\n",
    "\n",
    "history = embed_ms.fit(padded_train_model_seq_reshaped, y_train, epochs=40)\n",
    "\n",
    "#embed_ms.save('w'+str(sentence_length)+'_l'+str(sequence_length)+'_no_max_pooling_with_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc04129",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_ms = keras.models.load_model('w1_l8000_no_max_pooling_no_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    " \n",
    "evaluation = pd.DataFrame(columns=['loss', 'accuracy', 'w', 'l'])\n",
    "i=1\n",
    "for x in os.listdir():\n",
    "    if \"no_max_pooling_no_embedding\" in x:\n",
    "        saved_ms = keras.models.load_model(x)\n",
    "        l = x.split('_')\n",
    "        window = int(l[0][1:])\n",
    "        length = int(l[1][1:])\n",
    "        padded_test_model_seq = pad_sequences(test_model_sequences, maxlen=length, padding='pre', truncating='post')\n",
    "        padded_test_model_seq_reshaped = np.reshape(padded_test_model_seq, (padded_test_model_seq.shape[0], length//window, window, 1))\n",
    "        score = saved_ms.evaluate(padded_test_model_seq_reshaped, y_test, verbose=1)\n",
    "        evaluation.loc[len(evaluation)]=score + [window] + [length]\n",
    "        print(i)\n",
    "        print(x, window, length)\n",
    "        i+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f12d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "padded_all_seq = pad_sequences(all_sequences, maxlen=8000, padding='pre', truncating='post')\n",
    " \n",
    "documents = [','.join(x) for x in padded_all_seq.astype(str)]\n",
    "# Create a Vectorizer Object\n",
    "vectorizer = CountVectorizer()\n",
    " \n",
    "vectorizer.fit(documents)\n",
    "\n",
    "binary_labels = vectorized_traces['binary_label']\n",
    "#X_train, X_test, y_train, y_test, train_index, test_index = train_test_split(documents, binary_labels, vectorized_traces.index.to_list(), test_size=0.20, random_state=42, stratify=binary_labels) \n",
    " \n",
    "# Printing the identified Unique words along with their indices\n",
    "#print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    " \n",
    "# Encode the Document\n",
    "documents = [','.join(x) for x in padded_all_seq.astype(str)]\n",
    "\n",
    "vector = vectorizer.transform(documents)\n",
    "\n",
    "binary_labels = vectorized_traces['binary_label']\n",
    "X_train_cf, X_test_cf, y_train_cf, y_test_cf, train_index_cf, test_index_cf = train_test_split(vector, binary_labels, vectorized_traces.index.to_list(), test_size=0.20, random_state=42, stratify=binary_labels) \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24179c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# MNB CLASSIFICATION\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_cf,y_train_cf)\n",
    "mnb_prediction = mnb.predict(X_test_cf)\n",
    "\n",
    "mnb_results = np.array(list(zip(y_test_cf,mnb_prediction)))\n",
    "mnb_results = pd.DataFrame(mnb_results, columns=['id', 'result'])\n",
    "mnb_results.to_csv('mnb_vectorized.csv', index = False)\n",
    "\n",
    "print(classification_report(y_test_cf, mnb_prediction))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "acc_score = accuracy_score(y_test_cf,mnb_prediction)\n",
    "conf_mat = confusion_matrix(y_test_cf, mnb_prediction)\n",
    "print(acc_score)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7f289d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation.sort_values(['loss', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dfdea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow import keras\n",
    "\n",
    "ms = keras.models.load_model('w80_l8000_no_max_pooling_no_embedding')\n",
    "\n",
    "#padded_test_model_seq = pad_sequences(test_model_sequences, maxlen=8000, padding='pre', truncating='post')\n",
    "padded_test_model_seq_reshaped = np.reshape(padded_test_model_seq, (padded_test_model_seq.shape[0], 8000//80, 80, 1))\n",
    "\n",
    "y_pred_lstm = ms.predict(padded_test_model_seq_reshaped)\n",
    "\n",
    "lstm = pd.DataFrame()                                                             \n",
    "lstm['pred'] = y_pred_lstm.ravel()\n",
    "lstm['true'] = y_test.to_list()  \n",
    "\n",
    "lstm_exploit = lstm[lstm['true']==0].copy()\n",
    "\n",
    "lstm_exploit_1  = pd.DataFrame()\n",
    "lstm_exploit_1['true'] = 1- lstm_exploit['true']\n",
    "lstm_exploit_1['pred'] = 1- lstm_exploit['pred']\n",
    "lstm_exploit = pd.concat([lstm_exploit,lstm_exploit_1]) \n",
    "                                                                   \n",
    "fpr_lstm_0, tpr_lstm_0, threshold_lstm_0 = roc_curve(lstm_exploit['true'], lstm_exploit['pred'])\n",
    "auc_lstm_0 = auc(fpr_lstm_0, tpr_lstm_0)\n",
    "\n",
    "lstm_normal = lstm[lstm['true']==1].copy()\n",
    "\n",
    "lstm_normal_0  = pd.DataFrame()\n",
    "lstm_normal_0['true'] = 1- lstm_normal['true']\n",
    "lstm_normal_0['pred'] = 1- lstm_normal['pred']\n",
    "lstm_normal = pd.concat([lstm_normal,lstm_normal_0])\n",
    "                                                                    \n",
    "fpr_lstm_1, tpr_lstm_1, threshold_lstm_1 = roc_curve(lstm_normal['true'], lstm_normal['pred'])\n",
    "auc_lstm_1 = auc(fpr_lstm_1, tpr_lstm_1) \n",
    "                                                                    \n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_lstm_0, tpr_lstm_0, label='exploit (area = {:.4f})'.format(auc_lstm_0))\n",
    "plt.plot(fpr_lstm_1, tpr_lstm_1, label='normal (area = {:.4f})'.format(auc_lstm_1))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('Two Stage LSTM without Embedding')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "lstm_acc_score = accuracy_score(y_test, y_pred_lstm.ravel().round())\n",
    "lstm_conf_mat = confusion_matrix(y_test, y_pred_lstm.ravel().round())\n",
    "print(lstm_acc_score)\n",
    "print(lstm_conf_mat)\n",
    "\n",
    "# Supervised transformation based on random forests\n",
    "rf = RandomForestClassifier(max_depth=3, n_estimators=10)\n",
    "rf.fit(X_train_cf, y_train_cf)\n",
    "#mnb = MultinomialNB()\n",
    "#mnb.fit(X_train,y_train)\n",
    "#mnb_prediction = mnb.predict(X_test)\n",
    "y_pred_rf = rf.predict_proba(X_test_cf)[:, 1]\n",
    "\n",
    "rf = pd.DataFrame()                                                             \n",
    "rf['pred'] = y_pred_rf.ravel()\n",
    "rf['true'] = y_test_cf.to_list()  \n",
    "\n",
    "rf_exploit = rf[rf['true']==0].copy()\n",
    "\n",
    "rf_exploit_1  = pd.DataFrame()\n",
    "rf_exploit_1['true'] = 1- rf_exploit['true']\n",
    "rf_exploit_1['pred'] = 1- rf_exploit['pred']\n",
    "rf_exploit = pd.concat([rf_exploit,rf_exploit_1]) \n",
    "                                                                   \n",
    "fpr_rf_0, tpr_rf_0, threshold_rf_0 = roc_curve(rf_exploit['true'], rf_exploit['pred'])\n",
    "auc_rf_0 = auc(fpr_rf_0, tpr_rf_0)\n",
    "\n",
    "rf_normal = rf[rf['true']==1].copy()\n",
    "\n",
    "rf_normal_0  = pd.DataFrame()\n",
    "rf_normal_0['true'] = 1- rf_normal['true']\n",
    "rf_normal_0['pred'] = 1- rf_normal['pred']\n",
    "rf_normal = pd.concat([rf_normal,rf_normal_0])\n",
    "                                                                    \n",
    "fpr_rf_1, tpr_rf_1, threshold_rf_1 = roc_curve(rf_normal['true'], rf_normal['pred'])\n",
    "auc_rf_1 = auc(fpr_rf_1, tpr_rf_1) \n",
    "                                                                \n",
    "plt.figure(2)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rf_0, tpr_rf_0, label='exploit (area = {:.4f})'.format(auc_rf_0))\n",
    "plt.plot(fpr_rf_1, tpr_rf_1, label='normal (area = {:.4f})'.format(auc_rf_1))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('Random Forest')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "#fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, mnb_prediction)\n",
    "#auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "rf_acc_score = accuracy_score(y_test_cf, y_pred_rf.ravel().round())\n",
    "rf_conf_mat = confusion_matrix(y_test_cf, y_pred_rf.ravel().round())\n",
    "print(rf_acc_score)\n",
    "print(rf_conf_mat)\n",
    "\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_cf,y_train_cf)\n",
    "mnb_prediction = mnb.predict(X_test_cf)\n",
    "y_pred_mnb = mnb.predict_proba(X_test_cf)[:, 1]\n",
    "\n",
    "mnb = pd.DataFrame()                                                             \n",
    "mnb['pred'] = y_pred_mnb.ravel()\n",
    "mnb['true'] = y_test_cf.to_list()  \n",
    "\n",
    "mnb_exploit = mnb[mnb['true']==0].copy()\n",
    "\n",
    "mnb_exploit_1  = pd.DataFrame()\n",
    "mnb_exploit_1['true'] = 1- mnb_exploit['true']\n",
    "mnb_exploit_1['pred'] = 1- mnb_exploit['pred']\n",
    "mnb_exploit = pd.concat([mnb_exploit,mnb_exploit_1]) \n",
    "                                                                   \n",
    "fpr_mnb_0, tpr_mnb_0, threshold_mnb_0 = roc_curve(mnb_exploit['true'], mnb_exploit['pred'])\n",
    "auc_mnb_0 = auc(fpr_mnb_0, tpr_mnb_0)\n",
    "\n",
    "mnb_normal = mnb[mnb['true']==1].copy()\n",
    "\n",
    "mnb_normal_0  = pd.DataFrame()\n",
    "mnb_normal_0['true'] = 1- mnb_normal['true']\n",
    "mnb_normal_0['pred'] = 1- mnb_normal['pred']\n",
    "mnb_normal = pd.concat([mnb_normal,mnb_normal_0])\n",
    "                                                                    \n",
    "fpr_mnb_1, tpr_mnb_1, threshold_mnb_1 = roc_curve(mnb_normal['true'], mnb_normal['pred'])\n",
    "auc_mnb_1 = auc(fpr_mnb_1, tpr_mnb_1) \n",
    "                                                                \n",
    "plt.figure(3)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_mnb_0, tpr_mnb_0, label='exploit (area = {:.4f})'.format(auc_mnb_0))\n",
    "plt.plot(fpr_mnb_1, tpr_mnb_1, label='normal (area = {:.4f})'.format(auc_mnb_1))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('Naive Bayes')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "mnb_acc_score = accuracy_score(y_test_cf, y_pred_mnb.ravel().round())\n",
    "mnb_conf_mat = confusion_matrix(y_test_cf, y_pred_mnb.ravel().round())\n",
    "print(mnb_acc_score)\n",
    "print(mnb_conf_mat)\n",
    "\n",
    "#saved_ms_lstm_embedded = keras.models.load_model('w100_l8000_no_max_pooling_with_embedding')\n",
    "\n",
    "#padded_test_model_seq = pad_sequences(test_model_sequences, maxlen=8000, padding='pre', truncating='post')\n",
    "#padded_test_model_seq_reshaped = np.reshape(padded_test_model_seq, (padded_test_model_seq.shape[0], 8000//100, 100, 1))\n",
    "\n",
    "y_pred_lstm_embedded = embed_ms.predict(padded_test_model_seq_reshaped)\n",
    "\n",
    "lstm_embedded = pd.DataFrame()                                                             \n",
    "lstm_embedded['pred'] = y_pred_lstm_embedded.ravel()\n",
    "lstm_embedded['true'] = y_test.to_list()  \n",
    "\n",
    "lstm_embedded_exploit = lstm_embedded[lstm_embedded['true']==0].copy()\n",
    "\n",
    "lstm_embedded_exploit_1  = pd.DataFrame()\n",
    "lstm_embedded_exploit_1['true'] = 1- lstm_embedded_exploit['true']\n",
    "lstm_embedded_exploit_1['pred'] = 1- lstm_embedded_exploit['pred']\n",
    "lstm_embedded_exploit = pd.concat([lstm_embedded_exploit,lstm_embedded_exploit_1]) \n",
    "                                                                   \n",
    "fpr_lstm_embedded_0, tpr_lstm_embedded_0, threshold_lstm_embedded_0 = roc_curve(lstm_embedded_exploit['true'], lstm_embedded_exploit['pred'])\n",
    "auc_lstm_embedded_0 = auc(fpr_lstm_embedded_0, tpr_lstm_embedded_0)\n",
    "\n",
    "lstm_embedded_normal = lstm_embedded[lstm_embedded['true']==1].copy()\n",
    "\n",
    "lstm_embedded_normal_0  = pd.DataFrame()\n",
    "lstm_embedded_normal_0['true'] = 1- lstm_embedded_normal['true']\n",
    "lstm_embedded_normal_0['pred'] = 1- lstm_embedded_normal['pred']\n",
    "lstm_embedded_normal = pd.concat([lstm_embedded_normal,lstm_embedded_normal_0])\n",
    "                                                                    \n",
    "fpr_lstm_embedded_1, tpr_lstm_embedded_1, threshold_lstm_embedded_1 = roc_curve(lstm_embedded_normal['true'], lstm_embedded_normal['pred'])\n",
    "auc_lstm_embedded_1 = auc(fpr_lstm_embedded_1, tpr_lstm_embedded_1) \n",
    "                                                                    \n",
    "plt.figure(4)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_lstm_embedded_0, tpr_lstm_embedded_0, label='exploit (area = {:.4f})'.format(auc_lstm_embedded_0))\n",
    "plt.plot(fpr_lstm_embedded_1, tpr_lstm_embedded_1, label='normal (area = {:.4f})'.format(auc_lstm_embedded_1))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('Two Stage lstm_embedded with Embedding')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "lstm_embedded_acc_score = accuracy_score(y_test, y_pred_lstm_embedded.ravel().round())\n",
    "lstm_embedded_conf_mat = confusion_matrix(y_test, y_pred_lstm_embedded.ravel().round())\n",
    "print(lstm_embedded_acc_score)\n",
    "print(lstm_embedded_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_ms = keras.models.load_model('w100_l8000_no_max_pooling_no_embedding')\n",
    "\n",
    "padded_test_model_seq = pad_sequences(test_model_sequences, maxlen=8000, padding='pre', truncating='post')\n",
    "padded_test_model_seq_reshaped = np.reshape(padded_test_model_seq, (padded_test_model_seq.shape[0], 8000//100, 100, 1))\n",
    "\n",
    "y_pred_keras = saved_ms.predict(padded_test_model_seq_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d75d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_exploit_1  = pd.DataFrame()\n",
    "lstm_exploit = lstm[lstm['true']==0].copy()\n",
    "lstm_exploit_1['true'] = 1- lstm_exploit['true']\n",
    "lstm_exploit_1['pred'] = 1- lstm_exploit['pred']\n",
    "\n",
    "lstm_normal = lstm[lstm['true']==1].copy()\n",
    "\n",
    "    \n",
    "lstm_exploit = pd.concat([lstm_exploit,lstm_exploit_1]) \n",
    "lstm_exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce84276",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_cf,y_train_cf)\n",
    "mnb_prediction = mnb.predict(X_test_cf)\n",
    "y_pred_mnb = mnb.predict_proba(X_test_cf)[:, 1]\n",
    "y_pred_mnb\n",
    "\n",
    "lstm_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c536a08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.sort_values(['accuracy', 'loss'])\n",
    "padded_test_model_seq_reshaped = np.reshape(padded_test_model_seq, (padded_test_model_seq.shape[0], 8000//100, 100, 1))\n",
    "\n",
    "ms.evaluate(padded_test_model_seq_reshaped, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417703b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "x = ['W=1', 'W=20', 'W=50', 'W=80']\n",
    "y1 = [20, 10, 30]\n",
    "y2 = [15, 25, 20]\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.35\n",
    "\n",
    "# Set the positions of the bars on the x-axis\n",
    "r1 = np.arange(len(x))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "\n",
    "# Create the bar plot\n",
    "plt.bar(r1, y1, color='blue', width=bar_width, edgecolor='white', label='Factor 1')\n",
    "plt.bar(r2, y2, color='orange', width=bar_width, edgecolor='white', label='Factor 2')\n",
    "\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xticks([r + bar_width/2 for r in range(len(x))], x)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0240ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Sample data\n",
    "\n",
    "sns.set_palette(\"pastel\")\n",
    "\n",
    "evaluation = pd.read_csv('lstm_evaluation_metrics.csv')\n",
    "\n",
    "\n",
    "evaluation.rename(columns={\"loss\": \"Accuracy\", \"accuracy\": \"Loss\", \"w\":\"Window Size\", \"l\": \"Sequence Length\"}, inplace=True)\n",
    "\n",
    "# Create a pandas dataframe\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "# Create a bar plot using seaborn\n",
    "#sns.barplot(x='Sequence Length', y='Accuracy', hue='Window Size', data=evaluation)\n",
    "\n",
    "sns.barplot(x='Sequence Length', y='Loss', hue='Window Size', data=evaluation, dodge=True)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_palette(\"pastel\")\n",
    "\n",
    "evaluation = pd.read_csv('lstm_evaluation_metrics.csv')\n",
    "\n",
    "\n",
    "evaluation.rename(columns={\"loss\": \"Accuracy\", \"accuracy\": \"Loss\", \"w\":\"Window Size\", \"l\": \"Sequence Length\"}, inplace=True)\n",
    "\n",
    "\n",
    "# Create a bar plot using seaborn\n",
    "sns.catplot(x='Sequence Length', y='Loss', hue='Window Size', data=evaluation, kind='bar', height=5, aspect=2)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_palette(\"pastel\")\n",
    "\n",
    "evaluation = pd.read_csv('lstm_evaluation_metrics.csv')\n",
    "\n",
    "\n",
    "evaluation.rename(columns={\"loss\": \"Accuracy\", \"accuracy\": \"Loss\", \"w\":\"Window Size\", \"l\": \"Sequence Length\"}, inplace=True)\n",
    "\n",
    "\n",
    "# Create a bar plot using seaborn\n",
    "sns.catplot(x='Sequence Length', y='Accuracy', hue='Window Size', data=evaluation, kind='bar', height=5, aspect=2)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
